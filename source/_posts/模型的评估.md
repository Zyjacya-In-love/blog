---
title: 模型的评估
date: 2016-04-15 15:46:09
toc: true
tags:
- 模型优化
categories:
  - 机器学习
---

记录机器学习中评估模型的方法和技巧, 但我坚信熟才能生巧, 嗯就这样

<!--more-->

### **训练集与测试集**

在模式识别（pattern recognition）与机器学习（machine learning）的相关研究中，经常会将数据集（dataset）分为训练集（training set）跟测试集（testing set）这两个子集，前者用以建立模型（model），后者则用来评估该模型对未知样本进行预测时的精确度，正规的说法是泛化能力（generalization ability）。怎么将完整的数据集分为训练集跟测试集，必须遵守如下要点：

1. **只有训练集才可以用在模型的训练过程中，测试集则必须在模型完成之后才被用来评估模型优劣的依据。**
2. **训练集中样本数量必须够多，一般至少大于总样本数的50%。**
3. **两组子集必须从完整集合中均匀取样。**

- 其中最后一点特别重要，均匀取样的目的是希望减少训练集/测试集与完整集合之间的偏差（bias），但却也不易做到。一般的作法是随机取样，当样本数量足够时，便可达到均匀取样的效果

### **查准率与查全率**

查准率是查询的精度(precision), 查全率也叫`召回率`(recall rate), 在二分类问题中可以用这两个指标来衡量模型的性能

举个文档检索的例子:

| 二分类  |   相关  |  不相关  |
|--------|--- -----|--------|
|  检索到 |  检索到的相关文档  |  检索到的不相关文档  |
| 未检索到 | 未检索到的相关文档  |  未检索到的不相关文档  |

`查准率P`为`检索到的相关文档`与`检索到的文档总数`的比值, 即在所有检索到的文档中相关文档数的比例
`查全率R`为`检索到的相关文档`与`数据库中相关文档总数`的比值, 即在所有相关文档中检索到的相关文档数的比例

  ![](/img/模型的评估/pr.png)
  
  - 如果P-R曲线发生了交叉, 一般难以断定谁更好, 比如图中的A和B
  
  - 如果P-R被包含, 比如图中的A和C, 那么A模型更好
  
  如图所示, 查全率和查准率是一对矛盾的变量, 查准率高时, 查全率偏低, 反之亦然, 通常使用$F1$值来权衡二者, 定义如下:
  
  $$F_1=\frac{2P*R}{P+R}$$
  
  - $F_1$本质上是查准率和查全率的调和平均, 与算术平均和几何平均相比, 调和平均更加重视小的值
  
  - 根据重视具体应用的重视程度不同, 可以在$P,R$前面加上相应的系数进行权衡

### **交叉验证法(Cross Validation)**

- `基本思想` : 尽可能保证数据分布的一致性, 一般通过分层采样将原始数据（dataset）分为k个子集，每次用k-1个子集的并集作为训练集（training set）, 余下的那个子集做为，另一部分做为验证集（validation set）,也叫测试集，这样就可以获得k组训练/测试数据, 就可以进行k次训练, 最后返回k个结果的均值.

- **具体实现过程**

  ![10折交叉验证](/img/模型的评估/cv.png)

  - 由图可知, 我们将原始数据通过分层抽样分为10组, 每次将其中一组作为测试集, 剩余九组作为训练集去训练模型,      这样得到10个模型的测试结果, 最后返回10个测试结果的`均值`

